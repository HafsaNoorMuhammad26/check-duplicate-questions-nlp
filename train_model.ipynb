{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6ef547-a7f1-453b-a0b7-e51c3e339c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a959ce78-1152-42e9-bdfb-926d33ca80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABBREVIATIONS = {\n",
    "    \"ml\": \"machine learning\",\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    \"nlp\": \"natural language processing\",\n",
    "    \"cv\": \"computer vision\",\n",
    "    \"dl\": \"deep learning\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd71417-99a9-4d33-a018-b3990015cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531bce30-d6f4-431c-910c-0747a3fdbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand abbreviations\n",
    "    for abbr, full in ABBREVIATIONS.items():\n",
    "        text = re.sub(rf'\\b{abbr}\\b', full, text)\n",
    "\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fee972c-62e2-4620-b4eb-23a30ccb6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data():\n",
    "    \"\"\"Create synthetic training data for demo purposes\"\"\"\n",
    "    print(\"Creating synthetic training data...\")\n",
    "    \n",
    "    # Sample questions\n",
    "    questions = [\n",
    "        # Similar pairs (label = 1) - 50 pairs\n",
    "        (\"What is machine learning?\", \"What is ML?\", 1),\n",
    "        (\"How to learn Python?\", \"Best way to learn Python programming\", 1),\n",
    "        (\"What is artificial intelligence?\", \"Define AI\", 1),\n",
    "        (\"How to cook pasta?\", \"What's the recipe for pasta?\", 1),\n",
    "        (\"Best programming language?\", \"Which programming language is best?\", 1),\n",
    "        (\"How to lose weight?\", \"Ways to reduce weight\", 1),\n",
    "        (\"What is climate change?\", \"Explain global warming\", 1),\n",
    "        (\"How to invest in stocks?\", \"Stock market investment tips\", 1),\n",
    "        (\"What is data science?\", \"Define data science\", 1),\n",
    "        (\"How to make coffee?\", \"Coffee making method\", 1),\n",
    "        (\"What is blockchain?\", \"Explain blockchain technology\", 1),\n",
    "        (\"How to meditate?\", \"Meditation techniques\", 1),\n",
    "        (\"What is COVID-19?\", \"Coronavirus definition\", 1),\n",
    "        (\"How to study effectively?\", \"Effective study methods\", 1),\n",
    "        (\"What is cloud computing?\", \"Cloud technology explained\", 1),\n",
    "        (\"How to bake a cake?\", \"Cake baking recipe\", 1),\n",
    "        (\"What is deep learning?\", \"Deep learning definition\", 1),\n",
    "        (\"How to write a resume?\", \"Resume writing tips\", 1),\n",
    "        (\"What is quantum computing?\", \"Quantum computer explanation\", 1),\n",
    "        (\"How to learn English?\", \"English learning methods\", 1),\n",
    "        (\"What is cryptocurrency?\", \"Digital currency explanation\", 1),\n",
    "        (\"How to start a business?\", \"Business startup guide\", 1),\n",
    "        (\"What is neural network?\", \"Neural networks explained\", 1),\n",
    "        (\"How to cook rice?\", \"Rice cooking method\", 1),\n",
    "        (\"What is big data?\", \"Big data analytics\", 1),\n",
    "        (\"How to improve memory?\", \"Memory enhancement techniques\", 1),\n",
    "        (\"What is IoT?\", \"Internet of Things explained\", 1),\n",
    "        (\"How to make tea?\", \"Tea preparation method\", 1),\n",
    "        (\"What is 5G technology?\", \"5G network explanation\", 1),\n",
    "        (\"How to exercise at home?\", \"Home workout routine\", 1),\n",
    "        (\"What is virtual reality?\", \"VR technology explained\", 1),\n",
    "        (\"How to save money?\", \"Money saving tips\", 1),\n",
    "        (\"What is cybersecurity?\", \"Cyber security definition\", 1),\n",
    "        (\"How to learn coding?\", \"Programming learning guide\", 1),\n",
    "        (\"What is augmented reality?\", \"AR technology explained\", 1),\n",
    "        (\"How to make pizza?\", \"Pizza recipe at home\", 1),\n",
    "        (\"What is machine vision?\", \"Computer vision explained\", 1),\n",
    "        (\"How to manage time?\", \"Time management techniques\", 1),\n",
    "        (\"What is robotics?\", \"Robotics engineering\", 1),\n",
    "        (\"How to grow plants?\", \"Plant growth tips\", 1),\n",
    "        (\"What is natural language processing?\", \"NLP explained\", 1),\n",
    "        (\"How to paint a room?\", \"Room painting guide\", 1),\n",
    "        (\"What is computer programming?\", \"Coding definition\", 1),\n",
    "        (\"How to swim?\", \"Swimming techniques\", 1),\n",
    "        (\"What is software engineering?\", \"Software development\", 1),\n",
    "        (\"How to cook chicken?\", \"Chicken recipes\", 1),\n",
    "        (\"What is data mining?\", \"Data extraction techniques\", 1),\n",
    "        (\"How to drive a car?\", \"Car driving lessons\", 1),\n",
    "        (\"What is web development?\", \"Website creation\", 1),\n",
    "        (\"How to make bread?\", \"Bread baking recipe\", 1),\n",
    "        \n",
    "        # Dissimilar pairs (label = 0) - 50 pairs\n",
    "        (\"What is machine learning?\", \"How to cook pizza?\", 0),\n",
    "        (\"Python programming basics\", \"Best pizza recipes\", 0),\n",
    "        (\"AI and its applications\", \"Weather today\", 0),\n",
    "        (\"Data science course\", \"Football match results\", 0),\n",
    "        (\"How to learn coding?\", \"Gardening tips for beginners\", 0),\n",
    "        (\"What is blockchain?\", \"Chicken curry recipe\", 0),\n",
    "        (\"Deep learning algorithms\", \"Car maintenance guide\", 0),\n",
    "        (\"Natural language processing\", \"Yoga exercises\", 0),\n",
    "        (\"Cloud computing services\", \"Cake decorating ideas\", 0),\n",
    "        (\"Quantum physics\", \"Coffee brewing techniques\", 0),\n",
    "        (\"Software development\", \"Football team rankings\", 0),\n",
    "        (\"Machine learning models\", \"Hair styling methods\", 0),\n",
    "        (\"Data analysis techniques\", \"Movie reviews\", 0),\n",
    "        (\"Artificial intelligence\", \"Music instruments list\", 0),\n",
    "        (\"Web development frameworks\", \"Travel destinations\", 0),\n",
    "        (\"Cybersecurity threats\", \"Cooking oil types\", 0),\n",
    "        (\"Big data analytics\", \"Pet care guide\", 0),\n",
    "        (\"IoT devices\", \"Fashion trends\", 0),\n",
    "        (\"Neural networks\", \"Home cleaning tips\", 0),\n",
    "        (\"Virtual reality headsets\", \"Book recommendations\", 0),\n",
    "        (\"Robotics engineering\", \"Dance styles\", 0),\n",
    "        (\"5G technology\", \"Restaurant reviews\", 0),\n",
    "        (\"Computer vision\", \"Interior design ideas\", 0),\n",
    "        (\"Cryptocurrency trading\", \"Fitness equipment\", 0),\n",
    "        (\"Data mining algorithms\", \"Car brands comparison\", 0),\n",
    "        (\"Augmented reality apps\", \"Weather forecasting\", 0),\n",
    "        (\"Quantum computing\", \"Cooking recipes\", 0),\n",
    "        (\"Network security\", \"Gardening tools\", 0),\n",
    "        (\"Database management\", \"Musical instruments\", 0),\n",
    "        (\"Operating systems\", \"Travel packing tips\", 0),\n",
    "        (\"Programming languages\", \"Movie genres\", 0),\n",
    "        (\"Software testing\", \"Coffee shop locations\", 0),\n",
    "        (\"Mobile app development\", \"Exercise routines\", 0),\n",
    "        (\"Computer hardware\", \"Recipe ingredients\", 0),\n",
    "        (\"Data structures\", \"Fashion accessories\", 0),\n",
    "        (\"Algorithms\", \"Home decor ideas\", 0),\n",
    "        (\"Web design\", \"Vacation spots\", 0),\n",
    "        (\"Cloud storage\", \"Cooking methods\", 0),\n",
    "        (\"Machine translation\", \"Music bands\", 0),\n",
    "        (\"Speech recognition\", \"Art techniques\", 0),\n",
    "        (\"Predictive analytics\", \"Shopping malls\", 0),\n",
    "        (\"Computer graphics\", \"Yoga poses\", 0),\n",
    "        (\"Information retrieval\", \"Restaurant menus\", 0),\n",
    "        (\"Recommender systems\", \"Car models\", 0),\n",
    "        (\"Data visualization\", \"Haircut styles\", 0),\n",
    "        (\"Text mining\", \"Gym exercises\", 0),\n",
    "        (\"Pattern recognition\", \"Recipe books\", 0),\n",
    "        (\"Expert systems\", \"Travel agencies\", 0),\n",
    "        (\"Fuzzy logic\", \"Coffee beans types\", 0),\n",
    "        (\"Genetic algorithms\", \"Movie theaters\", 0),\n",
    "        \n",
    "        # More nuanced examples for better learning - 20 pairs\n",
    "        (\"How to code in Java?\", \"Java programming tutorial\", 1),  # Similar\n",
    "        (\"Java vs Python\", \"Difference between Java and Python\", 1),  # Similar\n",
    "        (\"Java programming\", \"Making coffee with Java beans\", 0),  # Dissimilar (Java double meaning)\n",
    "        (\"Apple iPhone features\", \"Apple fruit nutrition\", 0),  # Dissimilar (Apple double meaning)\n",
    "        (\"What is Amazon AWS?\", \"Amazon rainforest facts\", 0),  # Dissimilar (Amazon double meaning)\n",
    "        (\"Python snake facts\", \"Python programming language\", 0),  # Dissimilar\n",
    "        (\"How to use Git?\", \"Version control with Git\", 1),  # Similar\n",
    "        (\"Git commands tutorial\", \"Learning Git basics\", 1),  # Similar\n",
    "        (\"Git for beginners\", \"Cooking git fish recipe\", 0),  # Dissimilar\n",
    "        (\"Bank account opening\", \"River bank erosion\", 0),  # Dissimilar (bank double meaning)\n",
    "        (\"Cloud storage services\", \"Cloud formation in sky\", 0),  # Dissimilar\n",
    "        (\"Mouse for computer\", \"Mouse animal facts\", 0),  # Dissimilar\n",
    "        (\"Keyboard shortcuts\", \"Musical keyboard notes\", 0),  # Dissimilar\n",
    "        (\"What is Twitter?\", \"Bird twittering sounds\", 0),  # Dissimilar\n",
    "        (\"Facebook social media\", \"Face book for drawing\", 0),  # Dissimilar\n",
    "        (\"Instagram photo sharing\", \"Instant telegram message\", 0),  # Dissimilar\n",
    "        (\"LinkedIn professional network\", \"Link in chain\", 0),  # Dissimilar\n",
    "        (\"Netflix streaming service\", \"Fishing net fixing\", 0),  # Dissimilar\n",
    "        (\"Uber ride service\", \"Super uber vehicle\", 0),  # Dissimilar\n",
    "        (\"Tesla electric cars\", \"Nikola Tesla inventor\", 1),  # Similar (both about Tesla)\n",
    "        \n",
    "        # More challenging semantic pairs - 15 pairs\n",
    "        (\"How to become rich?\", \"Ways to earn money\", 1),  # Similar\n",
    "        (\"Feeling sad today\", \"I am unhappy\", 1),  # Similar\n",
    "        (\"Happy birthday wishes\", \"Best birthday messages\", 1),  # Similar\n",
    "        (\"Global warming effects\", \"Climate change impact\", 1),  # Similar\n",
    "        (\"Healthy food choices\", \"Nutrition diet plan\", 1),  # Similar\n",
    "        (\"Study hard for exams\", \"Prepare for tests\", 1),  # Similar\n",
    "        (\"Save water daily\", \"Conserve water resources\", 1),  # Similar\n",
    "        (\"Learn new skills\", \"Acquire new abilities\", 1),  # Similar\n",
    "        (\"Time is valuable\", \"Time management important\", 1),  # Similar\n",
    "        (\"Exercise daily routine\", \"Workout regularly\", 1),  # Similar\n",
    "        (\"Read books everyday\", \"Daily reading habit\", 1),  # Similar\n",
    "        (\"Sleep early tonight\", \"Go to bed early\", 1),  # Similar\n",
    "        (\"Drink more water\", \"Stay hydrated always\", 1),  # Similar\n",
    "        (\"Eat fresh fruits\", \"Consume healthy fruits\", 1),  # Similar\n",
    "        (\"Walk for 30 minutes\", \"Take a walk daily\", 1),  # Similar\n",
    "        (\"Benefits of exercise\", \"Advantages of working out\", 1),\n",
    "        (\"Healthy diet plan\", \"Nutritional meal planning\", 1),\n",
    "        (\"Mental health awareness\", \"Importance of mental wellbeing\", 1),\n",
    "        (\"Yoga for beginners\", \"Starting yoga practice\", 1),\n",
    "        (\"Sleep improvement tips\", \"How to sleep better?\", 1),\n",
    "        (\"Stress management techniques\", \"Ways to reduce stress\", 1),\n",
    "        (\"Meditation benefits\", \"Advantages of meditating\", 1),\n",
    "        (\"Drink enough water\", \"Stay hydrated daily\", 1),\n",
    "\n",
    "        # Dissimilar pairs\n",
    "        (\"Healthy heart\", \"Heart shape drawing\", 0),\n",
    "        (\"Blood pressure monitor\", \"Monitor computer screen\", 0),\n",
    "        (\"Vitamin C benefits\", \"C programming language\", 0),\n",
    "        (\"Dental care\", \"Car engine care\", 0),\n",
    "        (\"Eye exercises\", \"Exercise equipment\", 0)\n",
    "    ]\n",
    "    \n",
    "    # Create more variations\n",
    "    augmented_data = []\n",
    "    for q1, q2, label in questions:\n",
    "        augmented_data.append([q1, q2, label])\n",
    "        # Add some variations\n",
    "        if label == 1:\n",
    "            augmented_data.append([q2, q1, label])  # Reverse order\n",
    "    \n",
    "    df = pd.DataFrame(augmented_data, columns=['question1', 'question2', 'is_duplicate'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a704d1-6846-4e4b-8ef4-d0ef25737d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"Extract features from question pairs\"\"\"\n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    # Preprocess questions\n",
    "    df['q1_clean'] = df['question1'].apply(preprocess_text)\n",
    "    df['q2_clean'] = df['question2'].apply(preprocess_text)\n",
    "    \n",
    "    # Basic text features\n",
    "    df['q1_len'] = df['q1_clean'].apply(len)\n",
    "    df['q2_len'] = df['q2_clean'].apply(len)\n",
    "    df['len_diff'] = abs(df['q1_len'] - df['q2_len'])\n",
    "    \n",
    "    df['q1_word_count'] = df['q1_clean'].apply(lambda x: len(x.split()))\n",
    "    df['q2_word_count'] = df['q2_clean'].apply(lambda x: len(x.split()))\n",
    "    df['word_count_diff'] = abs(df['q1_word_count'] - df['q2_word_count'])\n",
    "    \n",
    "    # Common words feature\n",
    "    df['common_words'] = df.apply(\n",
    "        lambda row: len(set(row['q1_clean'].split()) & set(row['q2_clean'].split())), axis=1\n",
    "    )\n",
    "    \n",
    "    # TF-IDF features\n",
    "\n",
    "    # TF-IDF (FIT ON ALL QUESTIONS)\n",
    "    tfidf = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    "    )\n",
    "\n",
    "    tfidf.fit(pd.concat([df['q1_clean'], df['q2_clean']]))\n",
    "\n",
    "    # Transform questions\n",
    "    q1_tfidf = tfidf.transform(df['q1_clean'])\n",
    "    q2_tfidf = tfidf.transform(df['q2_clean'])\n",
    "\n",
    "    # Pair-wise cosine similarity\n",
    "    df['cosine_similarity'] = [\n",
    "        cosine_similarity(q1_tfidf[i], q2_tfidf[i])[0][0]\n",
    "        for i in range(len(df))\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    feature_cols = ['q1_len', 'q2_len', 'len_diff', \n",
    "                    'q1_word_count', 'q2_word_count', 'word_count_diff',\n",
    "                    'common_words', 'cosine_similarity']\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    y = df['is_duplicate']\n",
    "    \n",
    "    return X, y, df, tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a8fe43-8c03-428a-ac5b-151b4a8e688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model():\n",
    "    \"\"\"Train the model and save it as .pkl file\"\"\"\n",
    "    print(\"Training model...\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    df_synth = create_synthetic_data()\n",
    "\n",
    "    # csv file read\n",
    "    df_csv = pd.read_csv(\"questions.csv\", nrows=50000)\n",
    "\n",
    "    # keep only required columns\n",
    "    df_csv = df_csv[['question1', 'question2', 'is_duplicate']]\n",
    "    df_csv.dropna(inplace=True)\n",
    "    df_csv['is_duplicate'] = df_csv['is_duplicate'].astype(int)\n",
    "\n",
    "    #  Combine both\n",
    "    df = pd.concat([df_synth, df_csv], ignore_index=True)\n",
    "\n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"Final training rows:\", len(df))\n",
    "    \n",
    "    print(\"CSV Loaded:\", df.shape)\n",
    "\n",
    "    \n",
    "    # Extract features\n",
    "    X, y, df_with_features, tfidf = extract_features(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Save the model and vectorizer\n",
    "    joblib.dump(model, 'models/duplicate_model.pkl')\n",
    "    joblib.dump(tfidf, 'models/tfidf_vectorizer.pkl')\n",
    "    \n",
    "    # Save feature columns for later use\n",
    "    feature_cols = list(X.columns)\n",
    "    joblib.dump(feature_cols, 'models/feature_columns.pkl')\n",
    "    \n",
    "    # Generate and save visualizations\n",
    "    generate_visualizations(model, X_test, y_test, y_pred, df_with_features)\n",
    "    \n",
    "    print(f\"\\nModel saved as 'models/duplicate_model.pkl'\")\n",
    "    print(f\"TF-IDF vectorizer saved as 'models/tfidf_vectorizer.pkl'\")\n",
    "    \n",
    "    return accuracy, model, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e952a4-e7ea-47c6-9983-71986f51ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualizations(model, X_test, y_test, y_pred, df):\n",
    "    \"\"\"Generate and save visualization plots\"\"\"\n",
    "    print(\"Generating visualizations...\")\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Not Duplicate', 'Duplicate'],\n",
    "                yticklabels=['Not Duplicate', 'Duplicate'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Feature Importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Similarity Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get cosine similarity for duplicate and non-duplicate pairs\n",
    "    duplicate_sims = df[df['is_duplicate'] == 1]['cosine_similarity']\n",
    "    non_duplicate_sims = df[df['is_duplicate'] == 0]['cosine_similarity']\n",
    "    \n",
    "    plt.hist(duplicate_sims, alpha=0.7, label='Duplicate Pairs', bins=20)\n",
    "    plt.hist(non_duplicate_sims, alpha=0.7, label='Non-Duplicate Pairs', bins=20)\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Similarity Distribution: Duplicate vs Non-Duplicate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/similarity_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Visualizations saved in 'models/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fa7629-74ec-424f-869a-256e7a51c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Duplicate Questions Detection Model Training\n",
      "==================================================\n",
      "Training model...\n",
      "Creating synthetic training data...\n",
      "Final training rows: 50226\n",
      "CSV Loaded: (50226, 3)\n",
      "Extracting features...\n",
      "\n",
      "Model Accuracy: 69.30%\n",
      "Generating visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_6948\\1952430290.py:30: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations saved in 'models/' directory\n",
      "\n",
      "Model saved as 'models/duplicate_model.pkl'\n",
      "TF-IDF vectorizer saved as 'models/tfidf_vectorizer.pkl'\n",
      "\n",
      "==================================================\n",
      "Classification Report:\n",
      "==================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Duplicate       0.75      0.77      0.76      6262\n",
      "    Duplicate       0.60      0.57      0.58      3784\n",
      "\n",
      "     accuracy                           0.69     10046\n",
      "    macro avg       0.67      0.67      0.67     10046\n",
      " weighted avg       0.69      0.69      0.69     10046\n",
      "\n",
      "\n",
      "Training completed successfully!\n",
      "Model files saved in 'models/' directory\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*50)\n",
    "    print(\"Duplicate Questions Detection Model Training\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    accuracy, model, X_test, y_test, y_pred = train_and_save_model()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=['Not Duplicate', 'Duplicate']))\n",
    "    \n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    print(f\"Model files saved in 'models/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183a316-e1d1-4401-b429-d2f995720152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
